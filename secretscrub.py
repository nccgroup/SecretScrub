# Released as open source by NCC Group Plc - https://www.nccgroup.com/
#
# Developed by:
#     Andrew Kisliakov (andrew.kisliakov@nccgroup.com)
#
# Project link: https://www.github.com/nccgroup/secretscrub/
#
# Released under AGPL-3.0. See LICENSE for more information.

import argparse
import functools
import getpass
import io
import logging
import os
import regex
import shutil
from string import Template
import sys
import tempfile
from ruamel.yaml import YAML

from sarif import loader as sarif_loader

from secretscrub_types import *
from secretscrub_report import SecretScrubReport, SecretScrubReportEncryption as ReportEncryption
from cq2sarif import cq_to_sarif
from ccs2sarif import ccs_to_sarif
from bindetect import bin_detect
from archive import unpack_archives, repack_archives

import util

# Deprecated: Legacy format
OLD_PLACEHOLDER_FORMAT = ' *** SECRET DETECTED - this line was redacted as it matched the ${tool} rule: ${rule} *** '
DEFAULT_PLACEHOLDER_FORMAT = "[REDACTED SECRET${_yaml}]"

TOOL_NAME_BINDETECT = 'BinDetect'
TOOL_NAME_CQ = 'cq'
TOOL_NAME_CCS = 'ccs'
TOOL_NAME_GITLEAKS = 'Gitleaks'
TOOL_NAME_TRIVY = 'Trivy'

def parse_args():
    parser = argparse.ArgumentParser(prog = __file__,
        description = 'Using a set of SARIF files generated by popular static analysis tools, redacts the secrets found within a given codebase.',
        epilog = 'DISCLAIMER: This tool makes a best efforts attempt to remove secrets in code based on the output of known third-party static analysis tools that are capable of detecting secrets. It cannot detect secrets on its own, nor will it account for any errors within the output produced by those tools. This tool cannot guarantee the complete elimination of all secrets from the code.',
        formatter_class=argparse.RawTextHelpFormatter)
    input_group = parser.add_mutually_exclusive_group(required=True)
    input_group.add_argument('-i', '--input', metavar='DIRECTORY', help='A directory containing a number of pre-generated SARIF files containing details of detected secrets')
    input_group.add_argument('-a', '--analyse-with', '--analyze-with', metavar='TOOL_LIST', help='A comma-separated list of tools to run')
    parser.add_argument('-x', '--process-archives', action='store_true', help='Extract the contents of any archives and search for secrets found there')
    parser.add_argument('-s', '--srcdir', required=True, metavar='DIRECTORY', help='The directory containing the original codebase from which the SARIF files were generated')
    parser.add_argument('-o', '--outdir', required=True, metavar='DIRECTORY', help='A directory to store a copy of the provided codebase with secrets redacted')
    parser.add_argument('--placeholder', metavar='TEXT', help='The placeholder to insert in place of all detected secrets. This can accept the following substitutions:\n- ${tool} The name of the tool used to detect the secret\n- ${rule} The name of the rule used to detect the secret\n- ${regex} The regular expression associated with the rule used to detect the secret\n- ${yaml} A YAML flow style structure containing (if known) only the names of the tool and the rule used to detect the secret\n- ${yaml_regex} A YAML flow style structure containing (if known) the names of the tool and rule and the regular expression used to detect the secret',
                        default=DEFAULT_PLACEHOLDER_FORMAT)
    parser.add_argument('-r', '--report', metavar='PATH', help='The path to a CSV file which will contain a report with details of all the redacted secrets')
    parser.add_argument('--report-encryption', metavar='FORMAT', help='The format to use when encrypting the report', 
                        type=ReportEncryption.argparse, 
                        choices=list(ReportEncryption), default=ReportEncryption.ZIP_AES256)
    parser.add_argument('-l', '--log-level', metavar='LEVEL', help='The logging level to use')
    args = parser.parse_args(args=None if sys.argv[1:] else ['--help'])
    return args

def main(args):
    init_logging(args.log_level)
    report = SecretScrubReport(args.report, args.report_encryption)
    encryption_key_prompt = report.prompt_encryption_key()
    if encryption_key_prompt:
        report.set_encryption_key(getpass.getpass(prompt=f'{encryption_key_prompt} : '))

    tmp_dir = None
    src_dir = args.srcdir
    try:
        if args.process_archives:
            logging.info(f'Archive extraction requested. Copying to intermediate temporary directory...')
            tmp_dir = util.os.path.realpath(tempfile.mkdtemp())
            new_src_dir =  util.os.path.join(tmp_dir, os.path.basename(src_dir))
            shutil.copytree(src_dir, new_src_dir)
            src_dir = new_src_dir

            logging.info(f'Extracting archives...')
            unpack_archives(src_dir, report)

        if args.analyse_with:
            logging.info(f'Analysis requested: {args.analyse_with}')
            input = run_analysis(args.analyse_with.split(','), src_dir)
        else:
            logging.info(f'No analysis requested')
            input = args.input

        for file_type, file_subdir, file_path in get_input_files(input):
            if file_type == 'sarif':
                process_sarif_file(file_path, file_subdir, src_dir, args.outdir, report, args.placeholder)

        copy_remaining_files(src_dir, args.outdir)
        if args.process_archives:
            logging.info(f'Updating archives...')
            repack_archives(args.outdir, report)
    finally:
        try:
            report.close()
            if report.path:
                logging.info(f'Report written to {report.path}')
            report = None
        except Exception as e:
            logging.error(f'Error closing report file')

        try:
            if tmp_dir:
                shutil.rmtree(tmp_dir)
        except Exception as e:
            logging.error(f'Error deleting temporary source directory: {e}')


def init_logging(loglevel):
    loglevel = loglevel.upper() if loglevel else 'INFO'
    if hasattr(logging, 'getLevelNamesMapping'):
        mapping = logging.getLevelNamesMapping()
        if not loglevel in mapping:
            raise(f'Unsupported log level: {loglevel}')
    
        logging.basicConfig(level = mapping[loglevel])

def run_analysis(tools, src_dir):
    result_dir = tempfile.mkdtemp()
    for tool in tools:
        if tool == 'trivy':
            run_trivy(src_dir, result_dir)
        elif tool == 'gitleaks':
            run_gitleaks(src_dir, result_dir)
        elif tool == 'cq':
            run_cq(src_dir, result_dir)
        elif tool == 'ccs':
            run_ccs(src_dir, result_dir)
        elif tool == 'bindetect':
            run_bindetect(src_dir, result_dir)

    return result_dir

def run_trivy(src_dir, result_dir):
    logging.info("Running analysis using Trivy...")
    trivy_exes = ['trivy']
    if util.is_windows():
        trivy_exes.append('trivy.bat')
    completed = util.run_tool(trivy_exes, 'fs', '--scanners', 'secret', '--format', 'sarif', '--output', os.path.join(result_dir, '50-trivy.sarif'), '.', cwd=src_dir)
    completed.check_returncode()

def run_gitleaks(src_dir, result_dir):
    logging.info("Running analysis using GitLeaks...")
    completed = util.run_tool('gitleaks', 'detect', '--no-git', '--exit-code', '0', '--source', '.', '--report-format', 'sarif', '--report-path', os.path.join(result_dir, '50-gitleaks.sarif'), cwd=src_dir)
    completed.check_returncode()

def run_cq(src_dir, result_dir):
    logging.info("Running analysis using cq...")
    cq_path = os.path.join(os.path.dirname(__file__), 'cq', 'cq.py')
    completed = util.run_tool('python', cq_path, '-p', '-v', '-c', '^cred._*', '-ns', '-sa', result_dir, cwd=src_dir)
    completed.check_returncode()
    cq_to_sarif(cq_path, src_dir, result_dir, os.path.join(result_dir, '99-cq.sarif'))

def run_ccs(src_dir, result_dir):
    logging.info("Running analysis using ccs...")
    ccs_path = os.path.join(os.path.dirname(__file__), 'ccs', 'ccs.py')
    ccs_output = util.run_tool('python', ccs_path, '-p', '-v', '-ns', '-sa', '-dupes', '-everything', result_dir, cwd=src_dir, return_output=True)
    ccs_to_sarif(ccs_path, src_dir, ccs_output, os.path.join(result_dir, '99-ccs.sarif'))

def run_bindetect(src_dir, result_dir):
    logging.info("Running analysis using BinDetect...")
    bin_detect(src_dir, os.path.join(result_dir, '70-bindetect.sarif'))
    
def get_input_files(path):
    path = os.path.normpath(path)
    if os.path.isfile(path):
        input_file = check_input_file(path)
        if input_file:
            yield input_file
    if os.path.isdir(path):
        for (dirpath, _, filenames) in os.walk(path):
            subdir_list = list(dirpath.split(os.sep)[len(path.split(os.sep)):])
            subdir = os.sep.join(subdir_list) if subdir_list else ""
            for fname in filenames:
                input_file = check_input_file(subdir, os.path.join(dirpath, fname))
                if input_file:
                    yield input_file

def check_input_file(subdir, path):
    if os.path.isfile(path):
        if path.endswith('.sarif'):
            return ('sarif', subdir, path)
    return None

def process_sarif_file(file_path, file_subdir, srcdir, outdir, report, placeholder_fmt):
    sarif = sarif_loader.load_sarif_file(file_path)
    for sarif_run in sarif.runs:
        tool_name = verify_tool(sarif_run)
        logging.info(f'{tool_name} : {file_path}')
        if tool_name:
            process_results(tool_name, file_path, file_subdir, srcdir, outdir, report, placeholder_fmt, sarif_run)

Tool_Dict = {
    'gitleaks' : TOOL_NAME_GITLEAKS,
    'trivy': TOOL_NAME_TRIVY,
    'cq' : TOOL_NAME_CQ,
    'ccs' : TOOL_NAME_CCS,
    'bindetect' : TOOL_NAME_BINDETECT
}
def verify_tool(sarif_run):
    sarif_tool = sarif_run.get_tool_name().lower()

    if not sarif_tool in Tool_Dict:
        logging.warning(f'Unrecognised tool: {sarif_tool}')
        return None

    return Tool_Dict[sarif_tool]

def process_results(tool_name, sarif_file_path, subdir, src_path, out_path, report, placeholder_fmt, sarif_run):
    sarif_rules = build_rules_dict(tool_name, sarif_run)
    sarif_results_by_file = build_results_by_file_dict(tool_name, sarif_run)
    for artifact_path in sarif_results_by_file:
        for sarif_result in sorted(sarif_results_by_file[artifact_path], 
                                   key=functools.cmp_to_key(compare_sarif_result), reverse=True):
            logging.info(f'{os.path.join(subdir, os.path.split(sarif_file_path)[1])}: {sarif_result.locations[0].artifact_path}:{sarif_result.locations[0].start_line}:{sarif_result.locations[0].start_column} {sarif_result.message_text} :')

            (content_list, status, message) = process_result(tool_name, sarif_rules, subdir, src_path, out_path, placeholder_fmt, artifact_path, sarif_result)
            report.log_result(sarif_result, content_list, status, message)
            
def process_result(tool_name, sarif_rules, subdir, src_path, out_path, placeholder_fmt, artifact_path, sarif_result):
    sarif_rule = sarif_rules.get(sarif_result.rule_id)
    if not sarif_rule:
        logging.warning(f'Unknown or unsupported rule: {sarif_result.rule_id}')
        return

    # If gitleaks, need to check whether it's in a commit or not
    if sarif_result.commit_sha:
        logging.warning(f'This secret was found in a git history. We may not be able to locate it in the current version')

    placeholder_text = generate_placeholder(placeholder_fmt, tool_name, sarif_rule)

    # TODO: For now, only one location supported per result. Assume the first one is it.
    loc = sarif_result.locations[0] if len(sarif_result.locations) else None
    if loc is None or loc.artifact_path is None:
        logging.warning('No file path found')
        return (None, 'NoFilePath', 'No file path found')

    is_binary = loc.byte_offset is not None

    artifact_path = os.path.join(subdir, loc.artifact_path) if subdir else loc.artifact_path
    artifact_out_path = os.path.normpath(os.path.join(out_path, artifact_path))
    artifact_in_path = artifact_out_path if os.path.exists(artifact_out_path) else os.path.normpath(os.path.join(src_path, artifact_path))

    if is_binary:
        try:
            with io.open(artifact_in_path, 'rb') as f:
                artifact_bytes = f.read()            
        except Exception as e:
            return (None, 'IOError', 'Error reading file')
        
        if not validate_location_binary(artifact_bytes, loc):
            return (None, 'SarifError', 'Secret location invalid')

        matches = list(sarif_result.detect_secret_spans(loc, artifact_bytes))
        if not matches:
            return (None, 'IncorrectBytes', 'Bytes found in file does not match that in the SARIF report.')

        content_list = []
        for match in matches:
            content = artifact_bytes[match[0]:match[1]]
            content_list.append(content)
            artifact_bytes = artifact_bytes[0:match[0]] + placeholder_text.encode('utf-8') + artifact_bytes[match[1]:]

        try:
            # Output the artifact, creating directories if necessary
            os.makedirs(os.path.dirname(artifact_out_path), exist_ok=True)
            with io.open(artifact_out_path, 'wb') as f:
                f.write(artifact_bytes)
        except Exception as e:
            return (None, 'IOError', 'Error writing file')

    else:
        try:
            # Read in the artifact
            with io.open(artifact_in_path, 'r', newline='') as f:
                artifact_lines = f.readlines()
        except Exception as e:
            return (None, 'IOError', 'Error reading file')

        if not validate_location_text(artifact_lines, loc):
            return (None, 'SarifError', 'Secret location invalid')

        if loc.start_line != loc.end_line:
            logging.warning(f'Secret spans multiple lines in: "{artifact_path}" ({sarif_rule.id}). This isn\'t yet fully supported for certain tool types.')

        joined_lines = ''.join(artifact_lines[loc.start_line:loc.end_line+1])

        matches = list(sarif_result.detect_secret_spans(loc, joined_lines))
        if not matches:
            return (None, 'IncorrectText', 'Text found in file does not match that in the SARIF report.')

        content_list = []
        for match in matches:
            content = joined_lines[match[0]:match[1]]
            content_list.append(content)
            num_newlines = content.count('\n')
            joined_lines = joined_lines[0:match[0]] + placeholder_text + ('\n' * num_newlines) + joined_lines[match[1]:]

        artifact_lines[loc.start_line] = joined_lines
        for line_no in range(loc.end_line, loc.start_line, -1):
            del artifact_lines[line_no]

        try:
            # Output the artifact, creating directories if necessary
            os.makedirs(os.path.dirname(artifact_out_path), exist_ok=True)
            with io.open(artifact_out_path, 'w', newline='') as f:
                f.writelines(artifact_lines)
        except Exception as e:
            return (None, 'IOError', 'Error writing file')

    return (content_list,'Scrubbed','Scrubbed')

def build_rules_dict(tool_name, sarif_run):
    rules_dict = {}
    for rule_dict in get_from_dict(sarif_run.run_data, ['tool','driver','rules'], []):
        sarif_rule = get_sarif_rule(tool_name, rule_dict)
        if sarif_rule.is_secret:
            rules_dict[sarif_rule.id] = sarif_rule
    return rules_dict

def build_results_by_file_dict(tool_name, sarif_run):
    results_by_file_dict = {}
    for result in sarif_run.get_results():
        result = get_sarif_result(tool_name, result)
        for loc in result.locations:
            if not loc.artifact_path:
                continue
            if not loc.artifact_path in results_by_file_dict:
                results_by_file_dict[loc.artifact_path] = []
            results_by_file_dict[loc.artifact_path].append(result)
    return results_by_file_dict

def validate_location_binary(b, loc):
    if loc.byte_offset is None: return False
    if loc.byte_offset < 0 or (loc.byte_length is not None and loc.byte_length < 0): return False
    if loc.byte_offset >= len(b): return False
    if loc.byte_length is not None and (loc.byte_offset + loc.byte_length) > len(b): return False
    return True

def validate_location_text(lines, loc):
    if loc.start_line is None or loc.end_line is None: return False
    if loc.start_line < 0 or loc.end_line < 0: return False
    if loc.start_line > loc.end_line: return False
    if loc.end_line >= len(lines): return False
    if loc.start_column is not None and (loc.start_column < 0 or loc.start_column > len(lines[loc.start_line])): return False
    if loc.end_column is not None and (loc.end_column < 0 or loc.end_column > len(lines[loc.end_line])): return False
    if (loc.start_line == loc.end_line and loc.start_column is not None and loc.end_column is not None and loc.start_column > loc.end_column): return False
    return True

def sanitise_placeholder(s):
    if s is None:
        return None

    return s.replace('"', '\u201d').replace("'",'\u2019').replace('\r',' ').replace('\n',' ')

def generate_placeholder(fmt, tool_name, sarif_rule):
    template = Template(fmt)
    detection = {}
    if tool_name: detection['Tool'] = tool_name
    if sarif_rule and sarif_rule.name: detection['Rule'] = sarif_rule.name
    detection_regex = detection.copy()
    if sarif_rule and sarif_rule.regex: detection_regex['Regex'] = sarif_rule.regex

    yaml_dumper = YAML(typ=['rt','string'])
    yaml_dumper.default_flow_style = True
    yaml = yaml_dumper.dump_to_string({'Detection': detection}).strip() if len(detection) > 0 else ''
    yaml_regex = yaml_dumper.dump_to_string({'Detection': detection_regex}).strip() if len(detection_regex) > 0 else ''
    return template.safe_substitute(tool = tool_name, 
                      rule = (sarif_rule.name if sarif_rule else ""), 
                      regex = (sarif_rule.regex if sarif_rule else ""), 
                      yaml = yaml,
                      yaml_regex = yaml_regex,
                      _yaml = (f' {yaml}' if yaml else ''),                   # Special placeholder which inserts a space before the YAML only if it exists
                      _yaml_regex = (f' {yaml_regex}' if yaml_regex else '')  # Special placeholder which inserts a space before the YAML only if it exists
                      )

def get_sarif_rule(tool_name, sarif_rule):
    if tool_name == TOOL_NAME_TRIVY:
        return TrivyRule(sarif_rule)
    if tool_name == TOOL_NAME_GITLEAKS:
        return GitleaksRule(sarif_rule)
    if tool_name == TOOL_NAME_CQ:
        return CqRule(sarif_rule)
    if tool_name == TOOL_NAME_CCS:
        return CcsRule(sarif_rule)
    if tool_name == TOOL_NAME_BINDETECT:
        return BinDetectRule(sarif_rule)
    raise Exception(f'Unexpected tool name: {tool_name}')

def get_sarif_result(tool_name, sarif_result):
    if tool_name == TOOL_NAME_TRIVY:
        return TrivyResult(sarif_result)
    if tool_name == TOOL_NAME_GITLEAKS:
        return GitLeaksResult(sarif_result)
    if tool_name == TOOL_NAME_CQ:
        return CqResult(sarif_result)
    if tool_name == TOOL_NAME_CCS:
        return CcsResult(sarif_result)
    if tool_name == TOOL_NAME_BINDETECT:
        return BinDetectResult(sarif_result)
    raise Exception(f'Unexpected tool name: {tool_name}')

def compare_sarif_result(r1, r2):
    if len(r1.locations) != len(r2.locations) or len(r1.locations) == None:
        return len(r1.locations) - len(r2.locations)
    
    loc1 = r1.locations[0]
    loc2 = r2.locations[0]

    if loc1.artifact_path != loc2.artifact_path:
        return -1 if loc1.artifact_path < loc2.artifact_path else 1

    if loc1.start_line != loc2.start_line:
        return -1 if loc1.start_line < loc2.start_line else 1

    if loc1.start_column != loc2.start_column:
        return -1 if loc1.start_column < loc2.start_column else 1

    return 0

def copy_remaining_files(src_dir, dest_dir):
    
    logging.info(f'Copying remaining unredacted files...')
    for root, dirs, files in os.walk(src_dir):
        if not root.startswith(src_dir):
            logging.error(f'Attempt to copy unexpected directory {root}')
        root = os.path.normpath(root[len(src_dir):]).lstrip(os.sep)

        # We won't copy anything inside a .git directory because secrets could get in that way.
        if '.git' in root.split(os.sep):
            continue

        os.makedirs(os.path.join(dest_dir, root), exist_ok = True)
        for dir in filter(lambda d: d != '.git', dirs):
            os.makedirs(os.path.join(dest_dir, root, dir), exist_ok = True)
        for file in files:
            if not os.path.exists(os.path.join(dest_dir, root, file)):
                try:
                    shutil.copy2(os.path.join(src_dir, root, file), os.path.join(dest_dir, root, file))
                except OSError as e:
                    logging.error(f'ERROR: {e}')

class CqRule(SarifRule):
     def __init__(self, sarif_rule):
        super().__init__(TOOL_NAME_CQ, sarif_rule)
        self.regex = get_from_dict(sarif_rule, ['shortDescription','text'])

class CcsRule(SarifRule):
     def __init__(self, sarif_rule):
        super().__init__(TOOL_NAME_CCS, sarif_rule)
        self.regex = get_from_dict(sarif_rule, ['shortDescription','text'])

class GitleaksRule(SarifRule):
     def __init__(self, sarif_rule):
        super().__init__(TOOL_NAME_GITLEAKS, sarif_rule)
        self.regex = get_from_dict(sarif_rule, ['shortDescription','text'])

class TrivyRule(SarifRule):
     def __init__(self, sarif_rule):
        super().__init__(TOOL_NAME_TRIVY, sarif_rule)
        self.name = f"{get_from_dict(sarif_rule, ['name'])} - {get_from_dict(sarif_rule, ['shortDescription','text'])}"
        self.is_secret = 'secret' in get_from_dict(sarif_rule, ['properties','tags'], [])

class BinDetectRule(SarifRule):
     def __init__(self, sarif_rule):
        super().__init__(TOOL_NAME_BINDETECT, sarif_rule)
        self.name = f"{get_from_dict(sarif_rule, ['name'])}"
        self.is_secret = 'secret' in get_from_dict(sarif_rule, ['properties','tags'], [])

class CqResult(SarifResult):
    def __init__(self, sarif_result):
        super().__init__(TOOL_NAME_CQ, sarif_result)

    def detect_secret_spans(self, loc, lines):
        while (snippet_pos := lines.rfind(loc.snippet_text)) > -1:
            yield (snippet_pos, snippet_pos + len(loc.snippet_text))
            lines = lines[:snippet_pos]

class CcsResult(SarifResult):
    def __init__(self, sarif_result):
        super().__init__(TOOL_NAME_CCS, sarif_result)

    def detect_secret_spans(self, loc, lines):
        while (snippet_pos := lines.rfind(loc.snippet_text)) > -1:
            yield (snippet_pos, snippet_pos + len(loc.snippet_text))
            lines = lines[:snippet_pos]

class GitLeaksResult(SarifResult):
    def __init__(self, sarif_result):
        super().__init__(TOOL_NAME_GITLEAKS, sarif_result)
        for loc in self.locations:
            # Adjust for Gitleaks' incorrect column numbers
            if loc.start_line > 0:
                loc.start_column = loc.start_column - 1

    def detect_secret_spans(self, loc, lines):
        end_column = (lines.strip('\r\n').rfind('\n') + 1) + loc.end_column
        text = lines[loc.start_column:end_column+1]

        # Found the text, now scrub all instances of it (there could be more than one!)
        while (snippet_pos := text.rfind(loc.snippet_text)) > -1:
            yield (loc.start_column + snippet_pos, loc.start_column + snippet_pos + len(loc.snippet_text))
            text = text[:snippet_pos]

class TrivyResult(SarifResult):
    def __init__(self, sarif_result):
        super().__init__(TOOL_NAME_TRIVY, sarif_result)
        # Trivy squirrels away the matched text deep within the message text. Retrieve it
        # here and drop it into the location.
        if self.message_text and len(self.locations) == 1:
            message_text_lines = self.message_text.split('\n')
            for item in message_text_lines:
                if self.locations[0].line_text is None:
                    key_value = item.split(':', 1)
                    if len(key_value) < 2:
                        continue
                    (item_key, item_value) = tuple(key_value)
                    if item_key == 'Type':
                        self.message = item_value.strip()
                    if item_key == 'Match':
                        self.locations[0].line_text = item_value[1:]
                else:
                    # Further lines below the the Match column are continuations of the match.
                    self.locations[0].line_text = self.locations[0].line_text + '\n' + item


    def detect_secret_spans(self, loc, line):
        # Trivy match output provides up to 30 characters before, and up to 20 charactes after the secret.
        # The secret itself is masked using '*' characters.
        line_text = self.locations[0].line_text
        for ast_match in regex.finditer(r'\*+', line_text):
            (ast_start, ast_end) = ast_match.span()

            line_start = line_text[0:ast_start].rfind('\n') + 1
            line_end = line_text[ast_end:].find('\r')
            if line_end == -1:
                line_end = line_text[ast_end:].find('\n')
            line_end = (ast_end + line_end) if line_end > -1 else len(line_text)

            secret_regex = (('^' if line_start > 0 else '')
                + __class__.make_regex_safe_string(line_text[line_start:ast_start]) 
                + '(' + ('.' * (ast_end - ast_start)) + ')'
                + __class__.make_regex_safe_string(line_text[ast_end:line_end])
                + ('$' if line_end < len(line_text) else ''))

            match = regex.search(secret_regex, line.rstrip('\r\n'))
            if match:
                yield (match.start(1), match.end(1))

    def make_regex_safe_string(s):
        return ''.join([ch if ch.isalnum() else '\\u{:04x}'.format(ord(ch)) for ch in s])

class BinDetectResult(SarifResult):
    def __init__(self, sarif_result):
        super().__init__(TOOL_NAME_BINDETECT, sarif_result)

    def detect_secret_spans(self, loc, b):
        yield (loc.byte_offset, (loc.byte_offset + loc.byte_length) if loc.byte_length is not None else (len(b) - loc.byte_offset))

if __name__ == '__main__':
    args = parse_args()
    main(args)